{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# def read_embeddings(bucket: str):\n",
    "#     \"\"\"Read all embeddings from parquet files\"\"\"\n",
    "#     all_dfs = []\n",
    "#     for i in range(5):  # Assuming 5 parts\n",
    "#         try:\n",
    "#             df = pq.read_table(f's3://kishan.murthy/thumbnail_scoring/part_{i}.parquet').to_pandas()\n",
    "#             df['embedding'] = df['embedding'].apply(np.array)\n",
    "#             all_dfs.append(df)\n",
    "#         except:\n",
    "#             break\n",
    "#     return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# df = read_embeddings('your-bucket-name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('s3://kishan.murthy/thumbnail_scoring/big_data_downloaded_embeddings_09_30_2024.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from psycopg2.extras import execute_values\n",
    "import queue\n",
    "import threading\n",
    "from dataclasses import dataclass\n",
    "from threading import Event\n",
    "import psycopg2\n",
    "from psycopg2.pool import ThreadedConnectionPool\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingItem:\n",
    "    video_id: str\n",
    "    s3_path: str\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "class PipelinedEmbeddingsLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        db_config,\n",
    "        s3_client=None,\n",
    "        download_workers: int = 8,\n",
    "        db_workers: int = 2,\n",
    "        batch_size: int = 1000,\n",
    "        queue_size: int = 2000\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.download_workers = download_workers\n",
    "        self.db_workers = db_workers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize queues with reasonable sizes\n",
    "        self.download_queue = queue.Queue(maxsize=queue_size)\n",
    "        self.db_queue = queue.Queue(maxsize=queue_size)\n",
    "        \n",
    "        # Initialize database connection pool\n",
    "        self.db_pool = ThreadedConnectionPool(\n",
    "            minconn=2,\n",
    "            maxconn=db_workers + 1,  # +1 for main thread\n",
    "            host=db_config.host,\n",
    "            port=db_config.port,\n",
    "            database=db_config.database,\n",
    "            user=db_config.user,\n",
    "            password=db_config.password\n",
    "        )\n",
    "        \n",
    "        # Control flags\n",
    "        self.stop_event = Event()\n",
    "        self.error_event = Event()\n",
    "        \n",
    "        # Progress tracking\n",
    "        self.total_items = len(df)\n",
    "        self.processed_items = 0\n",
    "        self.progress_lock = threading.Lock()\n",
    "        self.pbar = None\n",
    "    \n",
    "    def _download_worker(self):\n",
    "        \"\"\"Worker function to download embeddings from S3 and send directly to DB queue\"\"\"\n",
    "        while not self.stop_event.is_set():\n",
    "            try:\n",
    "                item = self.download_queue.get(timeout=1)\n",
    "                if item is None:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    bucket = item.s3_path.split('/')[2]\n",
    "                    key = '/'.join(item.s3_path.split('/')[3:])\n",
    "                    response = self.s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                    \n",
    "                    array_buffer = BytesIO(response['Body'].read())\n",
    "                    embedding_array = np.load(array_buffer)\n",
    "                    item.embedding = embedding_array[0]  # Take first vector\n",
    "                    \n",
    "                    # Send directly to DB queue\n",
    "                    self.db_queue.put(item, timeout=30)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {item.s3_path}: {str(e)}\")\n",
    "                finally:\n",
    "                    self.download_queue.task_done()\n",
    "                    \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Download worker error: {str(e)}\")\n",
    "                self.error_event.set()\n",
    "                break\n",
    "    \n",
    "    def _db_worker(self):\n",
    "        \"\"\"Worker function to insert embeddings into database\"\"\"\n",
    "        conn = self.db_pool.getconn()\n",
    "        batch = []\n",
    "        last_commit_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            while not self.stop_event.is_set():\n",
    "                try:\n",
    "                    # Force batch processing if it's been too long\n",
    "                    if batch and (time.time() - last_commit_time > 10):\n",
    "                        self._process_batch(conn, batch)\n",
    "                        batch = []\n",
    "                        last_commit_time = time.time()\n",
    "                    \n",
    "                    # Get item from queue\n",
    "                    item = self.db_queue.get(timeout=1)\n",
    "                    if item is None:\n",
    "                        break\n",
    "                    \n",
    "                    batch.append((item.video_id, None, item.embedding.tolist()))\n",
    "                    \n",
    "                    # Process batch if full\n",
    "                    if len(batch) >= self.batch_size:\n",
    "                        self._process_batch(conn, batch)\n",
    "                        batch = []\n",
    "                        last_commit_time = time.time()\n",
    "                    \n",
    "                    self.db_queue.task_done()\n",
    "                    \n",
    "                except queue.Empty:\n",
    "                    if batch:  # Process remaining items\n",
    "                        self._process_batch(conn, batch)\n",
    "                        batch = []\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"DB worker error: {str(e)}\")\n",
    "            self.error_event.set()\n",
    "        finally:\n",
    "            # Process any remaining items\n",
    "            if batch:\n",
    "                try:\n",
    "                    self._process_batch(conn, batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in final batch processing: {str(e)}\")\n",
    "            self.db_pool.putconn(conn)\n",
    "\n",
    "    def _process_batch(self, conn, batch):\n",
    "        \"\"\"Process a batch of items\"\"\"\n",
    "        if not batch:\n",
    "            return\n",
    "            \n",
    "        with conn.cursor() as cur:\n",
    "            execute_values(\n",
    "                cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO video_embeddings \n",
    "                (video_id, title_embedding, thumbnail_embedding)\n",
    "                VALUES %s\n",
    "                ON CONFLICT (video_id) DO UPDATE\n",
    "                SET thumbnail_embedding = EXCLUDED.thumbnail_embedding\n",
    "                \"\"\",\n",
    "                batch\n",
    "            )\n",
    "            conn.commit()\n",
    "        \n",
    "        # Update progress\n",
    "        with self.progress_lock:\n",
    "            self.processed_items += len(batch)\n",
    "            if self.pbar is not None:\n",
    "                self.pbar.update(len(batch))\n",
    "    \n",
    "    def _get_missing_videos(self) -> set:\n",
    "        \"\"\"Get video IDs that don't have embeddings in the database\"\"\"\n",
    "        conn = self.db_pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT v.video_id\n",
    "                    FROM videos v\n",
    "                    LEFT JOIN video_embeddings ve ON v.video_id = ve.video_id\n",
    "                    WHERE ve.video_id IS NULL\n",
    "                \"\"\")\n",
    "                missing_video_ids = {row[0] for row in cur.fetchall()}\n",
    "            return missing_video_ids\n",
    "        finally:\n",
    "            self.db_pool.putconn(conn)\n",
    "    \n",
    "    def populate_embeddings(self):\n",
    "        \"\"\"Main function to populate embeddings using pipeline architecture\"\"\"\n",
    "        try:\n",
    "            # Get missing videos\n",
    "            missing_video_ids = self._get_missing_videos()\n",
    "            videos_to_process = self.df[self.df['video_id'].isin(missing_video_ids)]\n",
    "            \n",
    "            if videos_to_process.empty:\n",
    "                print(\"No new videos to process\")\n",
    "                return\n",
    "            \n",
    "            total_videos = len(videos_to_process)\n",
    "            print(f\"Processing {total_videos} videos\")\n",
    "            \n",
    "            # Initialize progress bar\n",
    "            self.pbar = tqdm(total=total_videos, desc=\"Processing\")\n",
    "            \n",
    "            # Start download workers\n",
    "            download_threads = []\n",
    "            for _ in range(self.download_workers):\n",
    "                t = threading.Thread(target=self._download_worker)\n",
    "                t.daemon = True\n",
    "                t.start()\n",
    "                download_threads.append(t)\n",
    "            \n",
    "            # Start DB workers\n",
    "            db_threads = []\n",
    "            for _ in range(self.db_workers):\n",
    "                t = threading.Thread(target=self._db_worker)\n",
    "                t.daemon = True\n",
    "                t.start()\n",
    "                db_threads.append(t)\n",
    "            \n",
    "            # Feed download queue\n",
    "            for _, row in videos_to_process.iterrows():\n",
    "                if self.error_event.is_set():\n",
    "                    break\n",
    "                    \n",
    "                item = EmbeddingItem(\n",
    "                    video_id=row['video_id'],\n",
    "                    s3_path=row['embedding_s3_path']\n",
    "                )\n",
    "                self.download_queue.put(item, timeout=30)\n",
    "            \n",
    "            # Signal workers to stop\n",
    "            for _ in range(self.download_workers):\n",
    "                self.download_queue.put(None)\n",
    "            \n",
    "            # Wait for download queue to empty\n",
    "            self.download_queue.join()\n",
    "            \n",
    "            # Signal DB workers to stop\n",
    "            for _ in range(self.db_workers):\n",
    "                self.db_queue.put(None)\n",
    "            \n",
    "            # Wait for DB queue to empty\n",
    "            self.db_queue.join()\n",
    "            \n",
    "            # Wait for all threads\n",
    "            for t in download_threads + db_threads:\n",
    "                t.join(timeout=5)\n",
    "            \n",
    "            if self.error_event.is_set():\n",
    "                print(\"\\nProcessing stopped due to errors\")\n",
    "            else:\n",
    "                print(f\"\\nProcessed {self.processed_items} videos successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in populate_embeddings: {str(e)}\")\n",
    "            self.error_event.set()\n",
    "            \n",
    "        finally:\n",
    "            self.stop_event.set()\n",
    "            if hasattr(self, 'db_pool'):\n",
    "                self.db_pool.closeall()\n",
    "            \n",
    "            if self.pbar is not None:\n",
    "                self.pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.db import DatabaseConfig\n",
    "\n",
    "config = DatabaseConfig()\n",
    "loader = PipelinedEmbeddingsLoader(\n",
    "    df=df,\n",
    "    db_config=config,\n",
    "    download_workers=8,\n",
    "    db_workers=2,\n",
    "    batch_size=1000,\n",
    "    queue_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 89700 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 89700/89700 [19:13<00:00, 91.13it/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "loader.populate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
